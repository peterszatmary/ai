---
layout: default
title: Hyperparameters
permalink: /hyperparameters.html
tags: categorisation neural-network deep-learning
---

[Cost function]({{site.url}}{{site.prod}}/activation-function.html)

### Definition

> "A hyperparameter is a setting that affects the structure or operation of the neural network. "



Tuning hyperparameters is the primary way to build a network that provides accurate 
predictions for a certain problem. 


### Hyperparameters related to neural network structure

- **Number of hidden layers** usually 2 - 10
- **Dropout** ( technique of dropping out hidden and visible  units of network randomly to prevent overfitting (20 %). It doubles the number of iterations needed for converging the network) 
- [Activation function]({{site.url}}{{site.prod}}/activation-function.html)
- **Weights initialization** 
    - with 0. Similar to linear network than.
    - with random numbers close to 0. Most common practise.

### Hyperparameters related to training algorithm

- **Learning rate**  controls how much to change the model in response to the estimated error each time the model weights are updated.
- **How many times ( epochs ) training should be repeated**.
- **Iterations** (whole data divided by number of batches) *iteration * batches = epoch*
- **Batch size** (whole dataset divided to smaller chunks of data)
- **Optimizer** is an algorithm or method used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. [^1]
- **Epoch** (one whole learning cycle performed on a whole dataset)
- **Momentum** <span class="color:red">TODO</span>


### Methods of Hyperparameter Tuning

- it is crucial to set the hyperparamaters as good as possible.
- there are few methods how to optimalise hyperparameters to create accurate and performing neural network.

#### Cross-validation-dividing

- If your training set is small, you can use cross-validationâ€”dividing the training set into multiple groups, training 
 the model on each of the groups then validating it on the other groups
 
#### Manual hyperparameter tuning
 
- an experienced operator can guess parameter values that will achieve very high accuracy. This requires trial and error.

#### Grid search
- this involves systematically testing multiple values of each hyperparameter and retraining the model for each combination.

#### Random search

- a research study by [Bergstra and Bengio](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
 showed that using random hyperparameter values is actually more effective than manual search or grid search.
 
#### Bayesian optimization
 
- a method proposed [by Shahriari, et al,](https://drive.google.com/viewerng/viewer?url=https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf) which trains the
 model with different hyperparameter values over and over again, and tries to observe the shape of the function generated by different parameter values. 
 It then extends this function to predict the best possible values. This method provides higher accuracy than random search.
 
#### Other optimizations

- there are plenty other optimization algorithms
- for other optimizations read [this comprehensive list](https://medium.com/@mikkokotila/a-comprehensive-list-of-hyperparameter-optimization-tuning-solutions-88e067f19d9)
 
<hr />

##### Footnotes:

[^1]: [Optimizers](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)


